{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReTap - UPDRS-Tapping Assessment - Predictions\n",
    "\n",
    "This notebooks investigates optimal hand- and fingertapping algorithms as part of the \n",
    "ReTune-Dyskinesia project.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Loading packages and functions, defining paths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Python and external packages\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib.gridspec as gridspec\n",
    "\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "from scipy import stats\n",
    "from array import array\n",
    "import datetime as dt\n",
    "from dataclasses import  dataclass, field\n",
    "from itertools import compress\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python sys 3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]\n",
      "pandas 1.4.4\n",
      "numpy 1.23.3\n",
      "sci-py 1.9.1\n",
      "sci-kit learn 1.1.2\n"
     ]
    }
   ],
   "source": [
    "# check some package versions for documentation and reproducability\n",
    "print('Python sys', sys.version)\n",
    "print('pandas', pd.__version__)\n",
    "print('numpy', np.__version__)\n",
    "# print('mne_bids', mne_bids.__version__)\n",
    "# print('mne', mne.__version__)\n",
    "print('sci-py', scipy.__version__)\n",
    "print('sci-kit learn', sk.__version__)\n",
    "\n",
    "\n",
    "## developed with:\n",
    "# Python sys 3.9.7 (default, Sep 16 2021, 08:50:36) \n",
    "# [Clang 10.0.0 ]\n",
    "# pandas 1.3.4\n",
    "# numpy 1.20.3\n",
    "# mne_bids 0.9\n",
    "# mne 0.24.1\n",
    "# sci-py 1.7.1\n",
    "# sci-kit learn 1.0.1\n",
    "\n",
    "## Currently (own env) since 31.08.22\n",
    "# Python sys 3.9.12 (main, Jun  1 2022, 06:36:29) \n",
    "# [Clang 12.0.0 ]\n",
    "# pandas 1.4.3\n",
    "# numpy 1.21.5\n",
    "# sci-py 1.7.3\n",
    "# sci-kit learn 1.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# own functions\n",
    "from retap_utils import utils_dataManagement\n",
    "import retap_utils.get_datasplit as get_split\n",
    "\n",
    "import tap_predict.tap_pred_prepare as pred_prep\n",
    "import tap_plotting.retap_plot_clusters as plot_cluster"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Split development and hold-out-test data sets\n",
    "\n",
    "- Development data is used to train and test the model using iterative cross-validation\n",
    "- Hold-out test data is NOT USED at all during cross-validation, and will be used to test the trained model as an external validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Import extracted Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORT CREATED CLASSES FROM FILES\n",
    "from tap_extract_fts.main_featExtractionClass import FeatureSet, singleTrace\n",
    "\n",
    "# define path with feature class\n",
    "deriv_path = os.path.join(utils_dataManagement.get_local_proj_dir(), 'data', 'derivatives')\n",
    "\n",
    "ftClass = utils_dataManagement.load_class_pickle(os.path.join(deriv_path, 'ftClass_ALL_20221214.P'))\n",
    "ftClass10 = utils_dataManagement.load_class_pickle(os.path.join(deriv_path, 'ftClass_ALL_max10_20221214.P'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in ALL DATA (BEFORE DATA SPLIT:\n",
      "# 18 included SUBS for BER\n",
      "# 311 included TRACES for BER\n",
      "# 19 included SUBS for DUS\n",
      "# 65 included TRACES for DUS\n"
     ]
    }
   ],
   "source": [
    "print('in ALL DATA (BEFORE DATA SPLIT:')\n",
    "subs = []\n",
    "for t in ftClass10.incl_traces:\n",
    "    subs.append(getattr(ftClass10, t).sub)\n",
    "\n",
    "unique_subs = list(set(subs))\n",
    "\n",
    "for cen in ['BER', 'DUS']:\n",
    "    n = sum([cen in t for t in unique_subs])\n",
    "    print(f'# {n} included SUBS for {cen}')\n",
    "    n = sum([cen in t for t in ftClass10.incl_traces])\n",
    "    print(f'# {n} included TRACES for {cen}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) ML-dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2a. Including ALL features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of NaNs per feat: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(pred_prep)\n",
    "\n",
    "traces, feats = pred_prep.select_traces_and_feats(\n",
    "    ftClass,\n",
    "    center='all',\n",
    "    use_sel_fts=True,\n",
    ")\n",
    "X, y = pred_prep.create_X_y_vectors(\n",
    "    ftClass,\n",
    "    incl_traces=traces,\n",
    "    incl_feats=feats,\n",
    "    to_norm=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Ensemble method, start with clustering on intraTapInterval and overall tapping-frequency\n",
    "\n",
    "Create X1 with selected input features (mean and coef of variation of intra-tap-interval) and\n",
    "overall tapping frequency to find two clusters (y_clusters) dividing fast vs slow tappers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPLITTING DATA IN DEV AND HOLD-OUT\n",
      "Original score distribution: {0: 40, 1: 154, 2: 122, 3: 57, 4: 3}\n",
      "Original score %: {0: 10.6, 1: 41.0, 2: 32.4, 3: 15.2, 4: 0.8}\n",
      "Accepted Split: random state 63\n",
      "\n",
      "Resulting distributions in splitted data sets:\n",
      "\n",
      "\tdev data set (n = 285):\n",
      "score 0: # 32 (11 %)\n",
      "score 1: # 115 (40 %)\n",
      "score 2: # 94 (33 %)\n",
      "score 3: # 42 (15 %)\n",
      "score 4: # 2 (1 %)\n",
      "\thout data set (n = 91):\n",
      "score 0: # 8 (9 %)\n",
      "score 1: # 39 (43 %)\n",
      "score 2: # 28 (31 %)\n",
      "score 3: # 15 (16 %)\n",
      "score 4: # 1 (1 %)\n",
      "# of NaNs per feat: [0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\habetsj\\Anaconda3\\envs\\retap\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean mean_intraTapInt (z-scored):\n",
      "\tcluster 0: -0.357289281961114\n",
      "\tcluster 1: 1.4546777908416781\n",
      "Fast tappers are clustered in cluster index 0\n",
      "Slow tappers are clustered in cluster index 1\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(pred_prep)\n",
    "importlib.reload(get_split)\n",
    "importlib.reload(plot_cluster)\n",
    "\n",
    "# set variables for pre-clustering\n",
    "ftClass_to_use = ftClass10\n",
    "n_clusters = 2\n",
    "traces_excl = [\n",
    "    'DUS006_M0S0_L_1',\n",
    "]\n",
    "ft_sel = [\n",
    "    'mean_intraTapInt',\n",
    "    'coefVar_intraTapInt',\n",
    "    'freq'\n",
    "]\n",
    "to_mask_4 = True\n",
    "to_mask_0 = False\n",
    "to_zscore = True\n",
    "to_norm = False\n",
    "\n",
    "# get dict with dev and hold-out datasets\n",
    "datasplit_subs = get_split.find_dev_holdout_split(\n",
    "    feats=ftClass_to_use, )\n",
    "\n",
    "# create dataclass for clustering (input matrix, label vector)\n",
    "# only include dev, exclude hold-out\n",
    "dev_data = pred_prep.create_X_y_vectors(\n",
    "    ftClass=ftClass_to_use,\n",
    "    incl_feats=ft_sel,\n",
    "    incl_traces=ftClass_to_use.incl_traces,\n",
    "    excl_traces=traces_excl,\n",
    "    excl_subs=datasplit_subs['hout'],  # excl hold out data\n",
    "    to_zscore=to_zscore,\n",
    "    to_norm=to_norm,\n",
    "    to_mask_4=to_mask_4,\n",
    "    to_mask_0=to_mask_0,\n",
    "    return_ids=True,\n",
    "    as_class=True\n",
    ")\n",
    "\n",
    "# create cluster labels\n",
    "y_clust, centr_clust, _ = plot_cluster.get_kMeans_clusters(\n",
    "    X=dev_data.X,\n",
    "    n_clusters=n_clusters,\n",
    "    use_pca=True,\n",
    "    to_zscore=to_zscore,\n",
    "    to_norm=to_norm,\n",
    ")\n",
    "\n",
    "# Define which cluster contains faster tappers\n",
    "cluster_mean_ITIs = []\n",
    "\n",
    "ft = 'mean_intraTapInt'\n",
    "if to_zscore: print(f'Mean {ft} (z-scored):')\n",
    "elif to_norm: print(f'Mean {ft} (normed):')\n",
    "\n",
    "for i_cls in np.unique(y_clust):\n",
    "\n",
    "    i_ft = np.where([f == ft for f in ft_sel])[0][0]\n",
    "    mean_iti_cluster = np.mean(dev_data.X[y_clust == i_cls, i_ft])\n",
    "    cluster_mean_ITIs.append(mean_iti_cluster)\n",
    "\n",
    "    print(f'\\tcluster {i_cls}: {mean_iti_cluster}')\n",
    "\n",
    "fast_cluster_i = np.argmin(cluster_mean_ITIs)\n",
    "if fast_cluster_i == 0: slow_cluster_i = 1\n",
    "if fast_cluster_i == 1: slow_cluster_i = 0\n",
    "\n",
    "print(f'Fast tappers are clustered in cluster index {fast_cluster_i}')\n",
    "print(f'Slow tappers are clustered in cluster index {slow_cluster_i}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make X_2 input Matrix with more features for score-prediction per Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of NaNs per feat: [0 0 0 0 3 0 0]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(pred_prep)\n",
    "\n",
    "incl_traces, ft_list = pred_prep.select_traces_and_feats(\n",
    "    ftClass_to_use, use_sel_fts=True, excl_traces=traces_excl,\n",
    ")\n",
    "\n",
    "feats_for_2nd_pred = [\n",
    "    # 'freq',\n",
    "    'coefVar_intraTapInt',\n",
    "    # 'mean_intraTapInt',\n",
    "    # 'slope_intraTapInt',\n",
    "    'decr_intraTapInt',\n",
    "    'mean_tapRMS',\n",
    "    'coefVar_tapRMS',\n",
    "    # 'slope_tapRMS',\n",
    "    'decr_tapRMS',\n",
    "    'mean_raise_velocity',\n",
    "    'jerkiness_trace'\n",
    "]\n",
    "\n",
    "cv_data = pred_prep.create_X_y_vectors(\n",
    "    ftClass_to_use,\n",
    "    incl_traces=ftClass_to_use.incl_traces,\n",
    "    incl_feats=feats_for_2nd_pred,\n",
    "    excl_traces=traces_excl,\n",
    "    excl_subs=datasplit_subs['hout'],  # due to hold out data set\n",
    "    to_norm=to_norm,\n",
    "    to_zscore=to_zscore,\n",
    "    to_mask_4=to_mask_4,\n",
    "    return_ids=True,\n",
    "    as_class=True,\n",
    ")\n",
    "\n",
    "# create final dataframe with true and ensemble-predicted labels\n",
    "# default all NaN's, filled during ensemble prediction\n",
    "overall_perf = pd.DataFrame(\n",
    "    data=np.array([[np.nan] * len(cv_data.y)] * 2).T,\n",
    "    columns=['y_true', 'y_pred'],\n",
    "    index=cv_data.ids,\n",
    ")\n",
    "overall_perf['y_true'] = cv_data.y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split input matrix X_2 in two generated clusters:\n",
    "- split X and y in two groups based on clusters\n",
    "- test default ML modeling on both groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CV X shape: (284, 7)\n",
      "Fast X shape: (228, 7), Slow X shape: (56, 7)\n"
     ]
    }
   ],
   "source": [
    "print(f'Total CV X shape: {cv_data.X.shape}')\n",
    "\n",
    "cv_fast_data = pred_prep.predictionData(\n",
    "    X=cv_data.X[y_clust == fast_cluster_i],\n",
    "    y=cv_data.y[y_clust == fast_cluster_i],\n",
    "    ids=cv_data.ids[y_clust == fast_cluster_i])\n",
    "\n",
    "cv_slow_data = pred_prep.predictionData(\n",
    "    X=cv_data.X[y_clust == slow_cluster_i],\n",
    "    y=cv_data.y[y_clust == slow_cluster_i],\n",
    "    ids=cv_data.ids[y_clust == slow_cluster_i])\n",
    "\n",
    "print(f'Fast X shape: {cv_fast_data.X.shape}, Slow X shape: {cv_slow_data.X.shape}')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise features in specific clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lists for boxplots of features per subscore, per cluster\n",
    "\n",
    "temp_data = cv_fast_data\n",
    "\n",
    "box_lists = {}\n",
    "for f in range(temp_data.X.shape[1]):\n",
    "    box_lists[f] = {}\n",
    "    for i in range(4): box_lists[f][i] = []\n",
    "\n",
    "\n",
    "for i in np.arange(temp_data.X.shape[0]):\n",
    "\n",
    "    score = temp_data.y[i]\n",
    "\n",
    "    for f in range(temp_data.X.shape[1]):\n",
    "\n",
    "        box_lists[f][int(score)].append(temp_data.X[i, f])\n",
    "\n",
    "# plot features within cluster, and decide on strategy\n",
    "# pm: use pre-knowledge about clusters\n",
    "# likelihood in faster cluster for 1-2 scores\n",
    "# use probabilities and adapt the threshold for acceptance\n",
    "# start finding border scores (e.g. 1 or 3)\n",
    "\n",
    "for i_f, ft in enumerate(feats_for_2nd_pred):\n",
    "\n",
    "    plot_lists = [box_lists[i_f][i] for i in range(4)]\n",
    "\n",
    "    plt.boxplot(plot_lists)\n",
    "    plt.title(ft)\n",
    "    plt.xticks(range(1, len(plot_lists) + 1), labels=['0', '1', '2', '3+4'])\n",
    "    plt.xlabel('UPDRS tap-score')\n",
    "    plt.ylabel('Z-score (a.u.)')\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test different prediction models for second step in Fast Cluster\n",
    "\n",
    "- Boolean Classification seemed inferior compared to MultiClass RF\n",
    "\n",
    "    optimal thresholds (to prevent too large False Positive Values)\n",
    "    predicting the best tappers (0-1)\n",
    "    - (best) RandomForest, cutoff .75 (TPR ~ .75-.8, FPR ~ .15)\n",
    "    - .58 - .6 for LogReg\n",
    "    - .6 for svm linear kernel\n",
    "    - .6 for svm poly kernel\n",
    "\n",
    "    indicating updrs 3 chance for next step\n",
    "    - .15 for log reg and lda (svc not succesful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retap_utils.plot_helpers import remove_duplicate_legend\n",
    "from tap_predict import retap_cv_models as cv_models\n",
    "from tap_plotting import plot_cv_folds as plot_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, roc_auc_score, roc_curve,\n",
    "    accuracy_score, f1_score, precision_score,\n",
    "    recall_score, plot_roc_curve, plot_confusion_matrix\n",
    ")\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import RocCurveDisplay, ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tn, fp, fn, tp = confusion_matrix(y_true,y_preds).ravel()\n",
    "# # add outcomes to dedicated lists\n",
    "# ['Accuracy'].append(accuracy_score(y_true,y_preds))\n",
    "# ['AUROC'].append(roc_auc_score(y_true, y_probas[1]))\n",
    "# ['F1_score'].append(f1_score(y_true,y_preds))\n",
    "# ['Precision'].append(precision_score(y_true, y_preds)) # precision/PPV\n",
    "# ['Recall'].append(recall_score(y_true, y_preds)) # sensitivity/recall/TPR\n",
    "# ['FPR'].append(fp / (fp+tn)) # false-positive, false-alarm rate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Fast Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(class_weight='balanced', min_samples_split=5,\n",
      "                       n_estimators=500, random_state=27)\n",
      "Fold 0: # of samples: train 171, test 57\n",
      "Fold 1: # of samples: train 171, test 57\n",
      "Fold 2: # of samples: train 171, test 57\n",
      "Fold 3: # of samples: train 171, test 57\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(cv_models)\n",
    "importlib.reload(plot_folds)\n",
    "\n",
    "# CLassification Settings\n",
    "temp_data = cv_fast_data  # data to use here\n",
    "score_to_predict = 1\n",
    "clf_choice = 'RF'\n",
    "nFolds = 4\n",
    "to_plot = True\n",
    "\n",
    "multiclass = True\n",
    "\n",
    "if multiclass:\n",
    "    y_pred_true = temp_data.y\n",
    "    mc_labels = ['0', '1', '2', '3-4']\n",
    "\n",
    "else:\n",
    "    if score_to_predict == 1:\n",
    "        y_pred_true = temp_data.y <= score_to_predict\n",
    "        plot_thresholds = [.65, .7, .75]\n",
    "        roc_title = f'Identify UPDRS 0/1 vs Rest ({clf_choice})'\n",
    "\n",
    "    elif score_to_predict == 3:\n",
    "        y_pred_true = temp_data.y == score_to_predict\n",
    "        plot_thresholds = [.25, .4, .5]\n",
    "        roc_title = f'Identify UPDRS 3-4 vs Rest ({clf_choice})'\n",
    "\n",
    "\n",
    "# chance = round(sum(y_pred_true) / len(y_pred_true), 3)\n",
    "# print(f'shape of X_dev: {temp_data.X.shape}')\n",
    "# print(f'predicting UPDRS score {score_to_predict}')\n",
    "# print(f'total y true: {sum(y_pred_true)}')\n",
    "# print(f'Chance level: {chance}')\n",
    "\n",
    "(y_pred_dict, y_proba_dict,\n",
    " y_true_dict, og_pred_idx\n",
    ") = cv_models.get_cvFold_predictions_dicts(\n",
    "    X_cv=temp_data.X,\n",
    "    y_cv=y_pred_true,\n",
    "    cv_method=StratifiedKFold,\n",
    "    n_folds=nFolds,\n",
    "    clf=clf_choice,\n",
    ")\n",
    "if to_plot and not multiclass: \n",
    "    plot_folds.plot_ROC_AUC_confMatrices_for_folds(\n",
    "        y_true_dict=y_true_dict,\n",
    "        y_proba_dict=y_proba_dict,\n",
    "        plot_thresholds=plot_thresholds,\n",
    "        roc_title=roc_title,\n",
    "    )\n",
    "if multiclass:\n",
    "    cm = cv_models.multiclass_conf_matrix(\n",
    "        y_true=y_true_dict, y_pred=y_pred_dict,\n",
    "        labels=mc_labels,\n",
    "    )\n",
    "    # print(f'\\nConfusion Matrix:\\n{cm}')\n",
    "    # mean_pen, std_pen, _ = cv_models.get_penalties_from_conf_matr(cm)\n",
    "    # print(f'mean UPDRS-penalty: {round(mean_pen, 2)}'\n",
    "    #         f' (+/- {round(std_pen, 2)})')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true_temp, y_pred_temp = [], []\n",
    "\n",
    "# for f in np.arange(len(y_true_dict)):\n",
    "#     y_true_temp.extend(y_true_dict[f])\n",
    "#     y_pred_temp.extend(y_pred_dict[f])\n",
    "\n",
    "# kappa(y_true_temp, y_pred_temp)\n",
    "# scipy.stats.spearmanr(y_true_temp, y_pred_temp)\n",
    "\n",
    "# jitt = np.random.uniform(low=-.15, high=0.15, size=len(y_true_temp))\n",
    "# jitt2 = np.random.uniform(low=-.15, high=0.15, size=len(y_true_temp))\n",
    "# plt.scatter(y_true_temp+jitt, y_pred_temp+jitt2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Slow Tapper Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Included # of traces: 56\n",
      "\tscore 1: # 20 (36 %)\n",
      "\tscore 2: # 16 (29 %)\n",
      "\tscore 3: # 20 (36 %)\n",
      "RandomForestClassifier(class_weight='balanced', min_samples_split=5,\n",
      "                       n_estimators=500, random_state=27)\n",
      "Fold 0: # of samples: train 37, test 19\n",
      "Fold 1: # of samples: train 37, test 19\n",
      "Fold 2: # of samples: train 38, test 18\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(cv_models)\n",
    "importlib.reload(plot_folds)\n",
    "\n",
    "# CLassification Settings\n",
    "temp_data = cv_slow_data  # data to use here\n",
    "clf_choice = 'RF'\n",
    "nFolds = 3\n",
    "mask_0 = True\n",
    "multiclass = True\n",
    "score_to_predict = 3\n",
    "\n",
    "y_model = temp_data.y.copy()\n",
    "if mask_0: # mask 0's to 1\n",
    "    y_model[y_model == 0] = 1\n",
    "    mc_labels = ['0-1', '2', '3-4']\n",
    "else:\n",
    "    mc_labels = ['0', '1', '2', '3-4']\n",
    "\n",
    "if not multiclass:\n",
    "    y_model = y_model == score_to_predict\n",
    "    to_plot = True\n",
    "\n",
    "    if score_to_predict == 1:\n",
    "        plot_thresholds = [.65, .7, .75]\n",
    "        roc_title = f'Identify UPDRS 0/1 vs Rest ({clf_choice})'\n",
    "\n",
    "    elif score_to_predict == 3:\n",
    "        plot_thresholds = [.25, .4, .5]\n",
    "        roc_title = f'Identify UPDRS 3-4 vs Rest ({clf_choice})'\n",
    "\n",
    "# print descriptives\n",
    "n_samples = len(temp_data.ids)\n",
    "print(f'Included # of traces: {n_samples}')\n",
    "y_scores, counts = np.unique(y_model, return_counts=True)\n",
    "for s, c in zip(y_scores, counts):\n",
    "    print(f'\\tscore {s}: # {c} ({round(c / n_samples * 100)} %)')\n",
    "\n",
    "\n",
    "(y_pred_dict, y_proba_dict,\n",
    " y_true_dict, og_pred_idx\n",
    ") = cv_models.get_cvFold_predictions_dicts(\n",
    "    X_cv=temp_data.X,\n",
    "    y_cv=y_model,\n",
    "    cv_method=StratifiedKFold,\n",
    "    n_folds=nFolds,\n",
    "    clf=clf_choice,\n",
    ")\n",
    "if to_plot and not multiclass: \n",
    "    plot_folds.plot_ROC_AUC_confMatrices_for_folds(\n",
    "        y_true_dict=y_true_dict,\n",
    "        y_proba_dict=y_proba_dict,\n",
    "        plot_thresholds=plot_thresholds,\n",
    "        roc_title=roc_title,\n",
    "    )\n",
    "if multiclass:\n",
    "    cm = cv_models.multiclass_conf_matrix(\n",
    "        y_true=y_true_dict, y_pred=y_pred_dict,\n",
    "        labels=mc_labels,\n",
    "    )\n",
    "    # print(f'\\nConfusion Matrix:\\n{cm}')\n",
    "    # mean_pen, std_pen, _ = cv_models.get_penalties_from_conf_matr(cm)\n",
    "    # print(f'mean UPDRS-penalty: {round(mean_pen, 2)}'\n",
    "    #         f' (+/- {round(std_pen, 2)})')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true_temp, y_pred_temp = [], []\n",
    "\n",
    "# for f in np.arange(len(y_true_dict)):\n",
    "#     y_true_temp.extend(y_true_dict[f])\n",
    "#     y_pred_temp.extend(y_pred_dict[f])\n",
    "\n",
    "# print(f'Kappa: {kappa(y_true_temp, y_pred_temp)}, '\n",
    "#       f'R: {scipy.stats.spearmanr(y_true_temp, y_pred_temp)}')\n",
    "\n",
    "# jitt = np.random.uniform(low=-.15, high=0.15, size=len(y_true_temp))\n",
    "# jitt2 = np.random.uniform(low=-.15, high=0.15, size=len(y_true_temp))\n",
    "# plt.scatter(y_true_temp+jitt, y_pred_temp+jitt2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test all Tappers (without clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(cv_models)\n",
    "importlib.reload(plot_folds)\n",
    "\n",
    "# CLassification Settings\n",
    "temp_data = cv_data  # data to use here\n",
    "clf_choice = 'RF'\n",
    "nFolds = 6\n",
    "mask_0 = False\n",
    "multiclass = True\n",
    "score_to_predict = 1\n",
    "to_plot=True\n",
    "\n",
    "y_model = temp_data.y.copy()\n",
    "if mask_0: # mask 0's to 1\n",
    "    y_model[y_model == 0] = 1\n",
    "    mc_labels = ['0-1', '2', '3-4']\n",
    "else:\n",
    "    mc_labels = ['0', '1', '2', '3-4']\n",
    "\n",
    "if not multiclass:\n",
    "    y_model = y_model == score_to_predict\n",
    "    to_plot = True\n",
    "\n",
    "    if score_to_predict == 1:\n",
    "        plot_thresholds = [.65, .7, .75]\n",
    "        roc_title = f'Identify UPDRS 0/1 vs Rest ({clf_choice})'\n",
    "\n",
    "    elif score_to_predict == 3:\n",
    "        plot_thresholds = [.25, .4, .5]\n",
    "        roc_title = f'Identify UPDRS 3-4 vs Rest ({clf_choice})'\n",
    "\n",
    "\n",
    "(y_pred_dict, y_proba_dict,\n",
    "y_true_dict, og_pred_idx\n",
    ") = cv_models.get_cvFold_predictions_dicts(\n",
    "    X_cv=temp_data.X,\n",
    "    y_cv=y_model,\n",
    "    cv_method=StratifiedKFold,\n",
    "    n_folds=nFolds,\n",
    "    clf=clf_choice,\n",
    "    verbose=False,\n",
    ")\n",
    "if to_plot and not multiclass: \n",
    "    plot_folds.plot_ROC_AUC_confMatrices_for_folds(\n",
    "        y_true_dict=y_true_dict,\n",
    "        y_proba_dict=y_proba_dict,\n",
    "        plot_thresholds=plot_thresholds,\n",
    "        roc_title=roc_title,\n",
    "        verbose=False,\n",
    "    )\n",
    "if multiclass:\n",
    "    cm = cv_models.multiclass_conf_matrix(\n",
    "        y_true=y_true_dict, y_pred=y_pred_dict,\n",
    "        labels=mc_labels,\n",
    "    )\n",
    "    # print(f'\\nConfusion Matrix:\\n{cm}')\n",
    "    # mean_pen, std_pen, pen_list = cv_models.get_penalties_from_conf_matr(cm)\n",
    "    # print(f'mean UPDRS-penalty: {round(mean_pen, 2)}'\n",
    "    #         f' (+/- {round(std_pen, 2)})')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true_temp, y_pred_temp = [], []\n",
    "\n",
    "# for f in np.arange(len(y_true_dict)):\n",
    "#     y_true_temp.extend(y_true_dict[f])\n",
    "#     y_pred_temp.extend(y_pred_dict[f])\n",
    "\n",
    "# print(f'Kappa: {kappa(y_true_temp, y_pred_temp)}, '\n",
    "#       f'R: {scipy.stats.spearmanr(y_true_temp, y_pred_temp)}')\n",
    "\n",
    "# jitt = np.random.uniform(low=-.15, high=0.15, size=len(y_true_temp))\n",
    "# jitt2 = np.random.uniform(low=-.15, high=0.15, size=len(y_true_temp))\n",
    "# plt.scatter(y_true_temp+jitt, y_pred_temp+jitt2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for Significance using Permutation Tests with shuffeld labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retap_utils.utils_dataManagement import find_onedrive_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A) Create Permutation Results based on random picking of outcome without Classification Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Penalty without distribution knowledge: 1.134\n",
      "Penalty alpha .05 cut off without distribution knowledge: 1.06\n"
     ]
    }
   ],
   "source": [
    "penalties_full_chance = []\n",
    "true_labels = overall_perf['y_true'].values\n",
    "\n",
    "r_states = np.linspace(0, 1000, n_permutations).astype(int)\n",
    "\n",
    "for r_seed in r_states:\n",
    "    np.random.seed(r_seed)\n",
    "    random_labels = np.random.randint(0, 3 + 1, size=len(true_labels))\n",
    "    diffs = abs(true_labels - random_labels)\n",
    "    penalties_full_chance.append(diffs.mean())\n",
    "\n",
    "print(\n",
    "    'Mean Penalty without distribution'\n",
    "    f' knowledge: {round(np.mean(penalties_full_chance), 3)}')\n",
    "print(\n",
    "    'Penalty alpha .05 cut off without distribution'\n",
    "    f' knowledge: {round(np.percentile(penalties_full_chance, 5), 3)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B) Create MultiClass Permutation Results based Classification of Shuffled or Random Labels\n",
    "\n",
    "- Optionally: Population-Matched (weighted) Distribution of UPDRS scores\n",
    "- TODO: Repeat with n_permutations = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start permutation iteration 0 (random seed 1)\n",
      "Start permutation iteration 1 (random seed 1489)\n",
      "Permutations succesfully saved as: PermErrors_CvDevData_RF_n2_randomLabels\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(cv_models)\n",
    "importlib.reload(plot_folds)\n",
    "\n",
    "\"\"\"\n",
    "Run permutation test\n",
    "    +/- 3-5 seconds per permutation round\n",
    "\"\"\"\n",
    "\n",
    "# Permutation settings\n",
    "random_base_seed = 27  # never changes, to ensure same results in randomisation\n",
    "n_permutations = 2\n",
    "match_label_distribution = False  # take same UPDRS score distribution as real labels\n",
    "to_save_perms = True\n",
    "# CV settings\n",
    "clf_choice = 'RF'\n",
    "nFolds = 6\n",
    "mask_0 = False\n",
    "\n",
    "\n",
    "# Perm settings\n",
    "np.random.seed(random_base_seed)\n",
    "perm_errors = {'mean': [], 'lists': []}\n",
    "r_states = np.random.choice(5000, size=n_permutations, replace=False)\n",
    "\n",
    "# CLassification Settings\n",
    "y_orig = cv_data.y.copy()  # multiclass labels 0-1-2-3(4)\n",
    "X_orig = cv_data.X.copy()\n",
    "\n",
    "if mask_0: # mask 0's to 1\n",
    "    y_model[y_model == 0] = 1\n",
    "    min_label = 1\n",
    "    mc_labels = ['0-1', '2', '3-4']\n",
    "else:\n",
    "    min_label = 0\n",
    "    mc_labels = ['0', '1', '2', '3-4']\n",
    "\n",
    "# Perform Permutations\n",
    "for n_prm, r_seed in enumerate(r_states):\n",
    "    print(f'Start permutation iteration {n_prm} (random seed {r_seed})')\n",
    "    np.random.seed(r_seed)\n",
    "    # Create random y-labels\n",
    "    if match_label_distribution:\n",
    "        # Create random y-labels with same distribution of scores\n",
    "        y_perm = y_orig.copy()  # copy true y-labels\n",
    "        np.random.shuffle(y_perm)  # shuffle true y-labels\n",
    "\n",
    "    elif not match_label_distribution:\n",
    "        # take random set of labels between 0 and (incl) 3\n",
    "        y_perm = np.random.randint(min_label, 3 + 1, size=len(true_labels))\n",
    "\n",
    "    # Perform random Classification\n",
    "    (y_pred_dict,\n",
    "     y_proba_dict,\n",
    "     y_true_dict,\n",
    "     og_pred_idx) = cv_models.get_cvFold_predictions_dicts(\n",
    "        X_cv=X_orig,\n",
    "        y_cv=y_perm,\n",
    "        cv_method=StratifiedKFold,\n",
    "        n_folds=nFolds,\n",
    "        clf=clf_choice,\n",
    "        verbose=False,\n",
    "    )\n",
    "    # Create MultiClass Conf Matrix\n",
    "    cm = cv_models.multiclass_conf_matrix(\n",
    "        y_true=y_true_dict, y_pred=y_pred_dict,\n",
    "        labels=mc_labels,\n",
    "    )\n",
    "    (mean_error,\n",
    "     std_error,\n",
    "     error_list) = cv_models.get_penalties_from_conf_matr(cm)\n",
    "    # Add permuted scores to lists\n",
    "    perm_errors['mean'].append(mean_error)\n",
    "    perm_errors['lists'].append(error_list)\n",
    "\n",
    "if to_save_perms:\n",
    "    save_dir = os.path.join(find_onedrive_path('results'),\n",
    "                            'predictions', 'permutations')\n",
    "    fname = f'PermErrors_CvDevData_{clf_choice}_n{n_permutations}'\n",
    "    if match_label_distribution: fname += '_weightedLabels'\n",
    "    else: fname += '_randomLabels'\n",
    "\n",
    "    if not os.path.exists(save_dir): os.makedirs(save_dir)\n",
    "\n",
    "    temp = np.array(perm_errors['lists'])\n",
    "    temp = pd.DataFrame(temp.T)  # rows are n-samples, columns are n-perms\n",
    "    # np.savetxt(\n",
    "    temp.to_csv(os.path.join(save_dir, f'{fname}.csv'),\n",
    "                sep=',', header=False, index=False,\n",
    "                float_format=np.float32)\n",
    "    print(f'Permutations succesfully saved as: {fname}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C) Load and Plot Prediction Results vs Permutation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retap_utils.utils_dataManagement import find_onedrive_path\n",
    "\n",
    "n_perm = 5\n",
    "alpha = .05\n",
    "save_fig=False\n",
    "\n",
    "# fname = f'PermErrors_CvDevData_{clf_choice}_n{n_permutations}'\n",
    "# if match_label_distribution: fname += '_weightedLabels'\n",
    "# else: fname += '_randomLabels'\n",
    "# fname = f'RF_full_dev_data_{n_perm}perms_means.csv'\n",
    "fname = 'PermErrors_CvDevData_RF_n5_weightedLabels.csv'\n",
    "perm_dir = os.path.join(\n",
    "    find_onedrive_path('results'),\n",
    "    'predictions', 'permutations')\n",
    "\n",
    "# get mean error per permutation iteration\n",
    "perm_error_lists = np.genfromtxt(os.path.join(perm_dir, fname), delimiter=',')\n",
    "if perm_error_lists.shape[0] == n_perm:\n",
    "    mean_errors = perm_error_lists.mean(axis=1)\n",
    "elif perm_error_lists.shape[1] == n_perm:\n",
    "    mean_errors = perm_error_lists.mean(axis=0)\n",
    "else:\n",
    "    raise ValueError('Incorrect # shape of loaded Permutation Errors')\n",
    "# get alpha significance border\n",
    "sign_05 = round(np.percentile(mean_errors, alpha * 100), 2)\n",
    "\n",
    "# Plot Results\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "plt.rc('font', size=14)\n",
    "\n",
    "ymin, ymax = ax.get_ylim()\n",
    "# plot Real cross-validation prediction mean\n",
    "ax.axvline(.74, lw=3, ymin=ymin, ymax=ymax, color='darkgreen',\n",
    "           label=f'mean (unclustered)\\ncross-val predictions')\n",
    "# plot permutation test results\n",
    "ax.hist(mean_errors, color='darkblue', hatch='/', alpha=.5,\n",
    "        label=f'balanced permutations -\\nprediction means (n={n_perm})',)\n",
    "# ax.hist(penalties_full_chance, color='purple', hatch='//', alpha=.2,\n",
    "#         label=f'unbalanced permutations -\\nprediction means (n={n_perm})',)\n",
    "ax.axvline(sign_05, ymin=ymin, ymax=ymax, color='darkblue',\n",
    "           ls='--', label=f'alpha=0.05\\n(balanced)')\n",
    "# ax.axvline(1.06, ymin=ymin, ymax=ymax, color='purple',\n",
    "#            ls='--', label=f'alpha=0.05\\n(unbalanced)')\n",
    "\n",
    "\n",
    "ax.set_xlabel('Mean Prediction Error (UPDRS tap-score)')\n",
    "ax.set_ylabel('observations (#)')\n",
    "ax.legend(frameon=False, fontsize=12, ncol=1,\n",
    "          bbox_to_anchor=(.90, .99), loc='upper left',)\n",
    "for side in ['right', 'top']:\n",
    "    getattr(ax.spines, side).set_visible(False)\n",
    "plt.xlim(.65, 1.3)\n",
    "plt.ylim(0, 50)\n",
    "plt.tight_layout()\n",
    "if save_fig:\n",
    "    fname='RF_prediction_vs_200permutations_test'\n",
    "    plt.savefig(\n",
    "        os.path.join(find_onedrive_path('figures'),\n",
    "                    'prediction', fname),\n",
    "        facecolor='w', dpi=150,)\n",
    "plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rating Agreements with Cohen's Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score as kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k_score = kappa(y_true, y_pred)  # 1 is perfect, 0 is chance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) PM Traditional Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Candidate vetors based on descriptives and concept\n",
    "    - nTaps\n",
    "    - freq\n",
    "    - upVelo sum [std-dev + coefVar]\n",
    "    - impact RMS [coefVar + stddev]\n",
    "    - tapRMS and impactRMS [sum]\n",
    "    - \n",
    "- include per run (array tap-features): sum, mean, stddev, trend_slope\n",
    "\n",
    "- Cluster on UPDRS 4?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MANOVA\n",
    "\n",
    "- normality assumption violated (Shapiro test highly significant)\n",
    "- for every a priori selected feature: present difference between sub-score-groups is a Kruskal-Wallis test (non-parametric One-Way ANOVA alternative)\n",
    "- differences between two sub groups within a feature is a non-parametric test of two groups of quantitative values (likely varying lengths): Mann-Whitney-U\n",
    "- in total: correct alpha for number of repeated measures on specific level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import shapiro\n",
    "# for col in np.arange(X.shape[1]):\n",
    "#     print(feats[col], shapiro(X[:, col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statsmodels.multivariate.manova import MANOVA\n",
    "\n",
    "# stat_data = np.concatenate([X, y.reshape((len(y), 1))], axis=1)\n",
    "# manova_df = pd.DataFrame(\n",
    "#     data=stat_data,\n",
    "#     columns=feats + ['subscore'],\n",
    "# )\n",
    "# maov = MANOVA.from_formula(\n",
    "#     'nTaps + freq + mean_intraTapInt + coefVar_intraTapInt + IQR_jerkiness +'\n",
    "#     ' mean_raise_velocity + mean_tapRMSnrm ~ subscore ',\n",
    "#     # 'mean_jerkiness_smooth + IQR_jerkiness_smooth ~ subscore',\n",
    "#     data=manova_df,\n",
    "# )\n",
    "# print(maov.mv_test())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import kruskal\n",
    "# importlib.reload(pred_prep)\n",
    "\n",
    "# mask_scores = True\n",
    "\n",
    "# traces, feats = pred_prep.select_traces_and_feats(\n",
    "#     ftClass,\n",
    "#     center=center_incl,\n",
    "#     use_sel_fts=sel_feats,\n",
    "# )\n",
    "# X, y = pred_prep.create_X_y_vectors(\n",
    "#     ftClass,\n",
    "#     incl_traces=traces,\n",
    "#     incl_feats=feats,\n",
    "#     to_norm=False,\n",
    "# )\n",
    "# n_groups = 5\n",
    "# if mask_scores:\n",
    "#     # UPDRS 4 -> 3 merge\n",
    "#     mask = y == 4\n",
    "#     y[mask] = 3\n",
    "#     # UPDRS 0 -> 1 merge\n",
    "#     mask = y == 0\n",
    "#     y[mask] = 1\n",
    "\n",
    "#     n_groups = 3\n",
    "\n",
    "# stat_data = np.concatenate([X, y.reshape((len(y), 1))], axis=1)\n",
    "# stat_df = pd.DataFrame(\n",
    "#     data=stat_data,\n",
    "#     columns=feats + ['subscore'],\n",
    "# )\n",
    "\n",
    "# stat_fts = [\n",
    "#     'freq', 'coefVar_intraTapInt', 'mean_jerkiness', 'coefVar_jerkiness',\n",
    "#     'mean_tapRMSnrm', 'coefVar_tapRMSnrm', 'slope_tapRMSnrm'\n",
    "# ]\n",
    "# alpha = .05 / len(stat_fts)\n",
    "# for ft in stat_fts:\n",
    "#     tempft = stat_df[~np.isnan(stat_df[ft])]\n",
    "\n",
    "    \n",
    "#     if mask_scores:\n",
    "#         groups = [\n",
    "#             tempft[ft][tempft['subscore'] == s].reset_index(drop=True)\n",
    "#             for s in np.arange(1, n_groups + 1)\n",
    "#         ]\n",
    "#         krusk_stat, krusk_p = kruskal(\n",
    "#             groups[0], groups[1], groups[2], \n",
    "#         )\n",
    "#     else:\n",
    "#         groups = [\n",
    "#             tempft[ft][tempft['subscore'] == s].reset_index(drop=True)\n",
    "#             for s in np.arange(n_groups)\n",
    "#         ]\n",
    "#         krusk_stat, krusk_p = kruskal(\n",
    "#             groups[0], groups[1], groups[2], \n",
    "#             groups[3], groups[4]\n",
    "#         )\n",
    "#     print(f'\\n{ft}: \\n\\tGroup level sign. difference (Kruskal'\n",
    "#         f' Test): {krusk_p < alpha} (p = {np.round(krusk_p, 6)})\\n')\n",
    "#     for g in np.arange(n_groups - 1):\n",
    "\n",
    "#         mnwu_rho, mnwu_p = mannwhitneyu(groups[g], groups[g + 1])\n",
    "#         print(f'\\tupdrs {g} vs {g + 1} sign, (Mann-Whitney-U): '\n",
    "#             f'{mnwu_p < (alpha / (n_groups - 1))} (p = {np.round(mnwu_p, 6)})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### additional methods for none multiclass prediction\n",
    "\n",
    "# importlib.reload(pred_prep)\n",
    "\n",
    "### Extract best tappers (0-1 predicted) from fast-tappers and classify remaining part\n",
    "\n",
    "# # Set Final Prediction best fast-tappers to 1\n",
    "# set_outcome = True\n",
    "# data_fast_01, data_fast_rest = pred_prep.split_dataset_on_pred_proba(\n",
    "#     orig_dataset=cv_fast_data,\n",
    "#     probas=y_proba_dict,\n",
    "#     og_indices=og_pred_idx,\n",
    "#     proba_thr=.75,\n",
    "# )\n",
    "# if set_outcome:\n",
    "#     for trace_id in data_fast_01.ids:\n",
    "#         overall_perf.at[trace_id, 'y_pred'] = 1\n",
    "\n",
    "\n",
    "#### Identify UPDRS 3 in remaining fast tappers\n",
    "\n",
    "# - use positive prediction for UPDRS III in previous step (which is not used for splitting data)\n",
    "# - opt logreg threshold for updrs-3: .18\n",
    "# - opt lda threshold for updrs-3: .3\n",
    "\n",
    "### INCLUDE UPDRS 3 LABELS FROM RPEVIOUS TEP\n",
    "\n",
    "# importlib.reload(cv_models)\n",
    "# n_samples = len(data_fast_rest.ids)\n",
    "# print(f'Remaining # of traces: {n_samples}')\n",
    "# y_scores, counts = np.unique(data_fast_rest.y, return_counts=True)\n",
    "# for s, c in zip(y_scores, counts):\n",
    "#     print(f'\\tscore {s}: # {c} ({round(c / n_samples * 100)} %)')\n",
    "\n",
    "\n",
    "# multiclass = True\n",
    "# clf_choice = 'rf'\n",
    "# to_plot=True\n",
    "# mask_0 = True\n",
    "\n",
    "# y_model = data_fast_rest.y.copy()\n",
    "# if mask_0: # mask 0's to 1\n",
    "#     y_model[y_model == 0] = 1\n",
    "#     mc_labels = ['0-1', '2', '3-4']\n",
    "# else:\n",
    "#     mc_labels = ['0', '1', '2', '3-4']\n",
    "\n",
    "# if not multiclass:\n",
    "#     y_model = y_model >= 2\n",
    "\n",
    "# plot_thresholds=[.5, .6, .7]\n",
    "# roc_title=f'Rest Fast-Tappers == UPDRS 2/3 ({clf_choice})'\n",
    "\n",
    "\n",
    "# (cv_pred, cv_proba, cv_true, cv_idx\n",
    "# ) = cv_models.get_cvFold_predictions_dicts(\n",
    "#     X_cv=data_fast_rest.X,\n",
    "#     y_cv=y_model,\n",
    "#     n_folds=3,\n",
    "#     clf=clf_choice,\n",
    "# )\n",
    "\n",
    "# if clf_choice == 'lda': thresh3 = .3\n",
    "# elif clf_choice == 'logreg': thresh3 = .3\n",
    "\n",
    "# ids_pos_3 = []\n",
    "# for fold in cv_proba:\n",
    "#     for i_prob, proba in enumerate(cv_proba[fold]):\n",
    "#         if proba[1] > thresh3:\n",
    "#             og_i = cv_idx[fold][i_prob]\n",
    "#             ids_pos_3.append(\n",
    "#                 data_fast_rest.ids[og_i]\n",
    "#             )\n",
    "\n",
    "# if to_plot and not multiclass: \n",
    "#     plot_folds.plot_ROC_AUC_confMatrices_for_folds(\n",
    "#         y_true_dict=cv_true,\n",
    "#         y_proba_dict=cv_proba,\n",
    "#         plot_thresholds=plot_thresholds,\n",
    "#         roc_title=roc_title,\n",
    "#         incl_mean_ROC=True,\n",
    "#     )\n",
    "# if multiclass:\n",
    "#     cm = cv_models.multiclass_conf_matrix(\n",
    "#         y_true=cv_true, y_pred=cv_pred,\n",
    "#         labels=mc_labels,\n",
    "#     )\n",
    "#     print(f'\\nConfusion Matrix:\\n{cm}')\n",
    "#     mean_pen, std_pen, _ = cv_models.get_penalties_from_conf_matr(cm)\n",
    "#     print(f'mean UPDRS-penalty: {round(mean_pen, 2)}'\n",
    "#             f' (+/- {round(std_pen, 2)})')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f23a308fb4398211655e9950c8371f856de701ec09eac61c96054832e4a49057"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
